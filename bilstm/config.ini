[Model]
gru = False ; If true, use GRU instead of LSTM
embedding_dim = 64 ; Dimension of embedding layer
hidden_dim = 64 ; BiLSTM hidden state dimensionality
num_layers = 2 ; How many BiLSTMs to stack on top of another
forget_gate_to_one = True ; If true, initialize forget gate bias to one, otherwise to zero

[Training]
lr = 1e-3
number_epochs = 100
weight_decay = 0 ; For positive weight_decay, AdamW is used (See https://arxiv.org/abs/1711.05101)
dropout = 0

[Data]
batch_size = 8
data_folder = "data/sponsor_nlp_data"
model_store_path = "data/models/"
model_name = "bilstm3"
model_vocab_store = "data/model_metadata/vocab"
config_store = "data/model_metadata/configs"
progress_store = "data/model_metadata/training_progress"

[Frontend]
template_location = "frontend/templates/inference_template.html"
css_location = "frontend/templates/inference_template.css"
results_location = "frontend/results"